# Data Build Tool (dbt)

This folder contains the complete dbt project for transforming fentanyl awareness data from raw sources into analytical models.

## ğŸ“ Structure

```
data_build_tool/
â”œâ”€â”€ dbt/                          # dbt project files
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ staging/              # Data cleaning models
â”‚   â”‚   â””â”€â”€ marts/                # Final analytical models
â”‚   â”œâ”€â”€ seeds/                     # Raw data files
â”‚   â”œâ”€â”€ tests/                     # Data quality tests
â”‚   â””â”€â”€ macros/                    # Reusable SQL macros
â”œâ”€â”€ dbt_project.yml               # dbt configuration
â”œâ”€â”€ packages.yml                   # Package dependencies
â”œâ”€â”€ package-lock.yml               # Locked package versions
â”œâ”€â”€ fentanyl_awareness.duckdb     # DuckDB database
â”œâ”€â”€ logs/                          # dbt execution logs
â”œâ”€â”€ target/                        # Compiled artifacts
â””â”€â”€ dbt_packages/                  # Installed packages
```

## ğŸš€ Getting Started

### Prerequisites
- Python 3.10+
- dbt-core
- dbt-duckdb

### Installation
```bash
# Install dbt and DuckDB adapter
pip install dbt-core dbt-duckdb

# Install package dependencies
dbt deps
```

### Running the Pipeline
```bash
# Load raw data
dbt seed

# Transform data
dbt run

# Test data quality
dbt test

# Generate documentation
dbt docs generate
dbt docs serve
```

## ğŸ“Š Data Models

### Staging Models
- `stg_cdc_wonder_fentanyl_deaths_final_1999_2020` - Historical CDC data
- `stg_cdc_wonder_fentanyl_deaths_final_2018_2023` - Recent CDC data
- `stg_cdc_wonder_fentanyl_deaths_provisional_2018_current` - Provisional data
- `stg_census_state_population` - Population estimates
- `stg_census_state_economic` - Economic indicators

### Mart Models
- `fact_fentanyl_deaths_over_time` - Final analytical dataset

## ğŸ”§ Configuration

### dbt_project.yml
Main configuration file defining:
- Project name and version
- Model configurations
- Test configurations
- Macro paths
- Seed configurations

### packages.yml
Defines external package dependencies:
- `dbt_utils` - Utility macros and tests

## ğŸ“ˆ Data Flow

```
Raw Data Sources â†’ Seeds â†’ Staging Models â†’ Mart Models â†’ Final Dataset
     â†“               â†“           â†“              â†“            â†“
CDC WONDER      â†’ CSV Files â†’ Cleaned Data â†’ Analytics â†’ CSV Export
Census Data     â†’ Seeds     â†’ Validated    â†’ Combined  â†’ Google Sheets
Other Sources  â†’ Raw Data  â†’ Standardized â†’ Enriched  â†’ Final Output
```

## ğŸ§ª Testing

### Built-in Tests
- **Unique tests** - Ensure primary key uniqueness
- **Not null tests** - Validate required fields
- **Accepted values** - Check value constraints
- **Relationships** - Verify foreign key integrity

### Custom Tests
- **Data freshness** - Check data recency
- **Completeness** - Validate data coverage
- **Consistency** - Cross-source validation

### Running Tests
```bash
# Run all tests
dbt test

# Run specific test
dbt test --select test_name

# Run tests for specific model
dbt test --select model_name
```

## ğŸ“ Documentation

### Model Documentation
Each model includes:
- **Description** - Purpose and business logic
- **Columns** - Field descriptions and types
- **Tests** - Data quality checks
- **Dependencies** - Upstream models

### Generating Docs
```bash
# Generate documentation
dbt docs generate

# Serve locally
dbt docs serve
```

## ğŸ” Querying Data

### Using DuckDB CLI
```bash
# Connect to database
duckdb fentanyl_awareness.duckdb

# List tables
.tables

# Query data
SELECT * FROM fact_fentanyl_deaths_over_time LIMIT 10;
```

### Using Python
```python
import duckdb

# Connect and query
conn = duckdb.connect('fentanyl_awareness.duckdb')
df = conn.execute('SELECT * FROM fact_fentanyl_deaths_over_time').fetchdf()
conn.close()
```

## ğŸ› ï¸ Development

### Adding New Models
1. Create SQL file in appropriate folder
2. Add model documentation
3. Define tests
4. Update schema.yml

### Adding New Tests
1. Create test SQL file
2. Reference in schema.yml
3. Run with `dbt test`

### Debugging
- Check logs in `logs/` folder
- Use `dbt compile` to check SQL
- Use `dbt parse` to validate syntax

## ğŸ“Š Performance

### Optimization Tips
- Use incremental models for large datasets
- Implement proper indexing strategies
- Monitor query performance
- Use materialized views for complex calculations

### Monitoring
- Review execution logs
- Monitor model run times
- Check test failures
- Validate data quality metrics

## ğŸ”„ Automation

The data engineering pipeline is automated via GitHub Actions:
- **Weekly runs** - Automated data refresh
- **Quality checks** - Automated testing
- **Documentation** - Auto-generated docs
- **Deployment** - Automated model deployment

## ğŸ“ Support

For issues with the data engineering pipeline:
1. Check the logs in `logs/` folder
2. Review model documentation
3. Run tests to identify issues
4. Check dbt documentation
5. Open a GitHub issue with details
